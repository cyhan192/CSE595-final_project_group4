{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\n\ndef load_and_inspect_data(file_path, n=5, max_field_length=50):\n    with open(file_path, 'r') as f:\n        data = json.load(f)\n    print(f\"Data type: {type(data)}\")\n    \n    if isinstance(data, list):\n        print(f\"Number of records: {len(data)}\")\n        print(\"Sample records:\")\n        for i, record in enumerate(data[:n]):\n            print(f\"Record {i + 1}:\")\n            for key, value in record.items():\n                if isinstance(value, str) and len(value) > max_field_length:\n                    value = value[:max_field_length] + \"...\"\n                print(f\"  {key}: {value}\")\n            print()\n    elif isinstance(data, dict):\n        print(f\"Keys: {list(data.keys())}\")\n        for key, entries in data.items():\n            print(f\"Sample from {key}:\")\n            if isinstance(entries, list):\n                for i, record in enumerate(entries[:n]):\n                    print(f\"  Record {i + 1}:\")\n                    for k, v in record.items():\n                        if isinstance(v, str) and len(v) > max_field_length:\n                            v = v[:max_field_length] + \"...\"\n                        print(f\"    {k}: {v}\")\n                    print()\n            else:\n                print(f\"  {entries}\")\n    else:\n        print(\"Unknown data structure.\")\n    return data\n\ndata_file = 'all_data/www_2s_new.json'\ndataset = load_and_inspect_data(data_file, n=2, max_field_length=30)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_and_extract_train_data(data, max_field_length=50, n=2):\n\n    train_data = []\n    for i, record in enumerate(data[:n]):  \n        print(f\"Record {i + 1}:\")\n        if \"train\" in record:\n            current_train_data = record[\"train\"]\n            train_data.extend(current_train_data)\n            for j, train_item in enumerate(current_train_data[:n]):  \n                print(f\"  Train Item {j + 1}:\")\n                example_id = train_item.get(\"example_id\", \"N/A\")\n                print(f\"    Example ID: {example_id}\")\n                \n                stories = train_item.get(\"stories\", [])\n                for k, story in enumerate(stories[:n]):  \n                    story_id = story.get(\"story_id\", \"N/A\")\n                    sentences = story.get(\"sentences\", [])\n                    truncated_sentences = [s[:max_field_length] + \"...\" if len(s) > max_field_length else s for s in sentences]\n                    print(f\"      Story {k + 1} (ID: {story_id}): Sentences: {truncated_sentences}\")\n        else:\n            print(\"  No 'train' key found in record.\")\n    return train_data\ntrain_data = preprocess_and_extract_train_data(dataset, max_field_length=30, n=2)\nprint(f\"Extracted {len(train_data)} train items.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T03:41:18.686254Z","iopub.execute_input":"2024-12-13T03:41:18.686637Z","iopub.status.idle":"2024-12-13T03:41:18.695956Z","shell.execute_reply.started":"2024-12-13T03:41:18.686592Z","shell.execute_reply":"2024-12-13T03:41:18.694901Z"}},"outputs":[{"name":"stdout","text":"Record 1:\n  Train Item 1:\n    Example ID: 0-C0\n      Story 1 (ID: 0): Sentences: ['Tom bought a new dustbin for t...', 'Tom threw a broken plate in th...', 'Tom got some soup from the fri...', 'Tom put the soup in the microw...', 'Tom ate the cold soup.']\n      Story 2 (ID: 0): Sentences: ['Tom bought a new dustbin for t...', 'Tom threw a broken plate in th...', 'Tom got some soup from the fri...', 'Tom put the soup in the microw...', 'Tom turned on the microwave.']\n  Train Item 2:\n    Example ID: 0-C1\n      Story 1 (ID: 0): Sentences: ['Tom bought a new dustbin for t...', 'Tom threw a broken plate in th...', 'Tom unplugged the microwave.', 'Tom put the soup in the microw...', 'Tom turned on the microwave.']\n      Story 2 (ID: 0): Sentences: ['Tom bought a new dustbin for t...', 'Tom threw a broken plate in th...', 'Tom got some soup from the fri...', 'Tom put the soup in the microw...', 'Tom turned on the microwave.']\nRecord 2:\n  Train Item 1:\n    Example ID: 0-O0\n      Story 1 (ID: 0): Sentences: ['Tom threw a broken plate in th...', 'Tom bought a new dustbin for t...', 'Tom got some soup from the fri...', 'Tom put the soup in the microw...', 'Tom turned on the microwave.']\n      Story 2 (ID: 0): Sentences: ['Tom bought a new dustbin for t...', 'Tom threw a broken plate in th...', 'Tom got some soup from the fri...', 'Tom put the soup in the microw...', 'Tom turned on the microwave.']\n  Train Item 2:\n    Example ID: 0-O1\n      Story 1 (ID: 0): Sentences: ['Tom bought a new dustbin for t...', 'Tom threw a broken plate in th...', 'Tom got some soup from the fri...', 'Tom turned on the microwave.', 'Tom put the soup in the microw...']\n      Story 2 (ID: 0): Sentences: ['Tom bought a new dustbin for t...', 'Tom threw a broken plate in th...', 'Tom got some soup from the fri...', 'Tom put the soup in the microw...', 'Tom turned on the microwave.']\nExtracted 3129 train items.\n","output_type":"stream"}],"execution_count":76},{"cell_type":"code","source":"def format_meta_tasks_dynamic(data, k_support=3, k_query=2):\n    tasks = []\n\n    for item in data:\n        # 获取当前记录中的 stories\n        stories = item.get(\"stories\", [])\n        total_stories = len(stories)\n\n        if total_stories < 2:\n            print(f\"Skipping task due to insufficient stories: {total_stories} available.\")\n            continue\n\n        dynamic_k_support = min(k_support, total_stories - 1)\n        dynamic_k_query = min(k_query, total_stories - dynamic_k_support)\n\n        support_set = stories[:dynamic_k_support]\n        query_set = stories[dynamic_k_support:dynamic_k_support + dynamic_k_query]\n\n        if not support_set or not query_set:\n            print(f\"Skipping task due to empty support or query set: {total_stories} available.\")\n            continue\n\n        tasks.append({\n            \"support\": [{\"text\": \" \".join(story[\"sentences\"]), \"label\": story[\"plausible\"]} for story in support_set],\n            \"query\": [{\"text\": \" \".join(story[\"sentences\"]), \"label\": story[\"plausible\"]} for story in query_set]\n        })\n\n    print(f\"Number of tasks: {len(tasks)}\")\n    return tasks\nmeta_tasks_dynamic = format_meta_tasks_dynamic(train_data)\n\nif meta_tasks_dynamic:\n    print(f\"Number of tasks: {len(meta_tasks_dynamic)}\")\n    print(f\"Sample task: {meta_tasks_dynamic[0]}\")\nelse:\n    print(\"No tasks were generated. Check the dataset or task formatting logic.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T04:03:35.019448Z","iopub.execute_input":"2024-12-13T04:03:35.019826Z","iopub.status.idle":"2024-12-13T04:03:35.044612Z","shell.execute_reply.started":"2024-12-13T04:03:35.019794Z","shell.execute_reply":"2024-12-13T04:03:35.043709Z"}},"outputs":[{"name":"stdout","text":"Number of tasks: 3129\nNumber of tasks: 3129\nSample task: {'support': [{'text': 'Tom bought a new dustbin for the kitchen. Tom threw a broken plate in the dustbin. Tom got some soup from the fridge. Tom put the soup in the microwave. Tom ate the cold soup.', 'label': False}], 'query': [{'text': 'Tom bought a new dustbin for the kitchen. Tom threw a broken plate in the dustbin. Tom got some soup from the fridge. Tom put the soup in the microwave. Tom turned on the microwave.', 'label': True}]}\n","output_type":"stream"}],"execution_count":98},{"cell_type":"code","source":"def tokenize_meta_tasks(tasks, tokenizer, max_length=128):\n    tokenized_tasks = []\n\n    for task in tasks:\n        # 支持集\n        support_inputs = tokenizer(\n            [example[\"text\"] for example in task[\"support\"]],\n            truncation=True,\n            padding=\"max_length\",\n            max_length=max_length,\n            return_tensors=\"pt\"\n        )\n        support_labels = torch.tensor([example[\"label\"] for example in task[\"support\"]], dtype=torch.long)\n\n        # 查询集\n        query_inputs = tokenizer(\n            [example[\"text\"] for example in task[\"query\"]],\n            truncation=True,\n            padding=\"max_length\",\n            max_length=max_length,\n            return_tensors=\"pt\"\n        )\n        query_labels = torch.tensor([example[\"label\"] for example in task[\"query\"]], dtype=torch.long)\n\n        tokenized_tasks.append({\n            \"support\": {\"inputs\": support_inputs, \"labels\": support_labels},\n            \"query\": {\"inputs\": query_inputs, \"labels\": query_labels}\n        })\n\n    return tokenized_tasks\n\nfrom transformers import BertTokenizer\n\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\ntokenized_meta_tasks = tokenize_meta_tasks(meta_tasks, tokenizer)\nprint(f\"Number of tokenized tasks: {len(tokenized_meta_tasks)}\")\nprint(f\"Sample tokenized task: {tokenized_meta_tasks[0] if tokenized_meta_tasks else 'No tokenized tasks'}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T04:09:49.325127Z","iopub.execute_input":"2024-12-13T04:09:49.326395Z","iopub.status.idle":"2024-12-13T04:09:55.521299Z","shell.execute_reply.started":"2024-12-13T04:09:49.326340Z","shell.execute_reply":"2024-12-13T04:09:55.520381Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Number of tokenized tasks: 3129\nSample tokenized task: {'support': {'inputs': {'input_ids': tensor([[  101,  3419,  4149,  1037,  2047,  6497,  8428,  2005,  1996,  3829,\n          1012,  3419,  4711,  1037,  3714,  5127,  1999,  1996,  6497,  8428,\n          1012,  3419,  2288,  2070, 11350,  2013,  1996, 16716,  1012,  3419,\n          2404,  1996, 11350,  1999,  1996, 18302,  1012,  3419,  8823,  1996,\n          3147, 11350,  1012,   102,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]])}, 'labels': tensor([False])}, 'query': {'inputs': {'input_ids': tensor([[  101,  3419,  4149,  1037,  2047,  6497,  8428,  2005,  1996,  3829,\n          1012,  3419,  4711,  1037,  3714,  5127,  1999,  1996,  6497,  8428,\n          1012,  3419,  2288,  2070, 11350,  2013,  1996, 16716,  1012,  3419,\n          2404,  1996, 11350,  1999,  1996, 18302,  1012,  3419,  2357,  2006,\n          1996, 18302,  1012,   102,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]])}, 'labels': tensor([True])}}\n","output_type":"stream"}],"execution_count":105},{"cell_type":"code","source":"import torch\nfrom transformers import BertForSequenceClassification, AdamW\n\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\noptimizer = AdamW(model.parameters(), lr=1e-5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T04:10:35.986666Z","iopub.execute_input":"2024-12-13T04:10:35.987009Z","iopub.status.idle":"2024-12-13T04:10:36.303445Z","shell.execute_reply.started":"2024-12-13T04:10:35.986979Z","shell.execute_reply":"2024-12-13T04:10:36.302493Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":108},{"cell_type":"code","source":"def train_meta_learning(tasks, model, optimizer, num_epochs=3, print_interval=1):\n    model.train()\n    loss_fn = torch.nn.CrossEntropyLoss()\n\n    for epoch in range(num_epochs):\n        total_loss = 0\n        total_accuracy = 0\n        task_count = 0\n\n        for task in tasks:\n            support_inputs = task[\"support\"][\"inputs\"]\n            support_labels = task[\"support\"][\"labels\"].long().to(device)\n\n            query_inputs = task[\"query\"][\"inputs\"]\n            query_labels = task[\"query\"][\"labels\"].long().to(device)\n\n            optimizer.zero_grad()\n            support_outputs = model(\n                input_ids=support_inputs[\"input_ids\"].to(device),\n                attention_mask=support_inputs[\"attention_mask\"].to(device),\n                token_type_ids=support_inputs[\"token_type_ids\"].to(device),\n            )\n            logits = support_outputs.logits\n            support_loss = loss_fn(logits, support_labels)\n            support_loss.backward()\n            optimizer.step()\n\n            with torch.no_grad():\n                query_outputs = model(\n                    input_ids=query_inputs[\"input_ids\"].to(device),\n                    attention_mask=query_inputs[\"attention_mask\"].to(device),\n                    token_type_ids=query_inputs[\"token_type_ids\"].to(device),\n                )\n                query_logits = query_outputs.logits\n                query_preds = torch.argmax(query_logits, dim=1)\n                query_accuracy = (query_preds == query_labels).float().mean().item()\n\n            total_loss += support_loss.item()\n            total_accuracy += query_accuracy\n            task_count += 1\n\n        avg_loss = total_loss / task_count\n        avg_accuracy = total_accuracy / task_count\n\n        if (epoch + 1) % print_interval == 0:\n            print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss:.4f}, Overall Accuracy: {avg_accuracy:.4f}\")\ntrain_meta_learning(tokenized_meta_tasks, model, optimizer, num_epochs=3)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T04:43:52.153567Z","iopub.execute_input":"2024-12-13T04:43:52.153959Z","iopub.status.idle":"2024-12-13T04:54:47.061138Z","shell.execute_reply.started":"2024-12-13T04:43:52.153918Z","shell.execute_reply":"2024-12-13T04:54:47.060151Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/3, Average Loss: 0.2405, Overall Accuracy: 0.7939\nEpoch 2/3, Average Loss: 0.2122, Overall Accuracy: 0.8114\nEpoch 3/3, Average Loss: 0.1675, Overall Accuracy: 0.8073\n","output_type":"stream"}],"execution_count":116}]}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d64e7070-65e5-4aa7-89f7-4889f6928218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98904bbf-ae78-4f97-bb92-5122677c76d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import spacy\n",
    "#!pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6adccff-3caf-4076-ad38-cad28c26990a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_val = 595 # Save random seed for reproducibility\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "366b9798-61b5-47ec-89a0-ab074ec3b1bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed examples:\n",
      "{\n",
      "  story_id: \n",
      "    13,\n",
      "  worker_id: \n",
      "    A32W24TWSWXW,\n",
      "  type: \n",
      "    None,\n",
      "  idx: \n",
      "    None,\n",
      "  aug: \n",
      "    False,\n",
      "  actor: \n",
      "    John,\n",
      "  location: \n",
      "    kitchen,\n",
      "  objects: \n",
      "    cabinet, counter, knife, pan, potato, pizza,\n",
      "  sentences: \n",
      "    [\n",
      "      John was getting the snacks ready for the party.\n",
      "      John opened the cabinet, took out a pan and put it on the counter.\n",
      "      John opened the fridge and got out the pizza.\n",
      "      John put the pizza on the pan and put them into the oven.\n",
      "      John took a knife and cut the hot pizza in eight slices.\n",
      "    ],\n",
      "  length: \n",
      "    5,\n",
      "  example_id: \n",
      "    13,\n",
      "  plausible: \n",
      "    True,\n",
      "  breakpoint: \n",
      "    -1,\n",
      "  confl_sents: \n",
      "    [],\n",
      "  confl_pairs: \n",
      "    [],\n",
      "  states: \n",
      "    [\n",
      "      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['snacks', 0], ['party', 0]], 'exist': [['snacks', 4], ['party', 2]], 'clean': [['snacks', 0], ['party', 0]], 'power': [['snacks', 0], ['party', 0]], 'functional': [['snacks', 2], ['party', 2]], 'pieces': [['snacks', 0], ['party', 0]], 'wet': [['snacks', 0], ['party', 0]], 'open': [['snacks', 0], ['party', 0]], 'temperature': [['snacks', 0], ['party', 0]], 'solid': [['snacks', 0], ['party', 0]], 'contain': [['snacks', 0], ['party', 0]], 'running': [['snacks', 0], ['party', 0]], 'moveable': [['snacks', 2], ['party', 2]], 'mixed': [['snacks', 0], ['party', 0]], 'edible': [['snacks', 0], ['party', 0]]}\n",
      "      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['counter', 0], ['pan', 7], ['cabinet', 0]], 'exist': [['counter', 2], ['pan', 2], ['cabinet', 2]], 'clean': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'power': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'functional': [['counter', 2], ['pan', 2], ['cabinet', 2]], 'pieces': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'wet': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'open': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'temperature': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'solid': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'contain': [['counter', 6], ['pan', 0], ['cabinet', 8]], 'running': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'moveable': [['counter', 0], ['pan', 2], ['cabinet', 0]], 'mixed': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'edible': [['counter', 0], ['pan', 0], ['cabinet', 0]]}\n",
      "      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['fridge', 0], ['pizza', 7]], 'exist': [['fridge', 2], ['pizza', 2]], 'clean': [['fridge', 0], ['pizza', 0]], 'power': [['fridge', 2], ['pizza', 0]], 'functional': [['fridge', 2], ['pizza', 2]], 'pieces': [['fridge', 0], ['pizza', 0]], 'wet': [['fridge', 0], ['pizza', 0]], 'open': [['fridge', 4], ['pizza', 0]], 'temperature': [['fridge', 0], ['pizza', 1]], 'solid': [['fridge', 0], ['pizza', 0]], 'contain': [['fridge', 8], ['pizza', 0]], 'running': [['fridge', 2], ['pizza', 0]], 'moveable': [['fridge', 2], ['pizza', 2]], 'mixed': [['fridge', 0], ['pizza', 0]], 'edible': [['fridge', 0], ['pizza', 0]]}\n",
      "      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['oven', 0], ['pizza', 3], ['pan', 6]], 'exist': [['oven', 2], ['pizza', 2], ['pan', 2]], 'clean': [['oven', 0], ['pizza', 0], ['pan', 0]], 'power': [['oven', 2], ['pizza', 0], ['pan', 0]], 'functional': [['oven', 2], ['pizza', 2], ['pan', 2]], 'pieces': [['oven', 0], ['pizza', 0], ['pan', 0]], 'wet': [['oven', 0], ['pizza', 0], ['pan', 0]], 'open': [['oven', 8], ['pizza', 0], ['pan', 0]], 'temperature': [['oven', 2], ['pizza', 0], ['pan', 0]], 'solid': [['oven', 0], ['pizza', 0], ['pan', 0]], 'contain': [['oven', 6], ['pizza', 0], ['pan', 4]], 'running': [['oven', 0], ['pizza', 0], ['pan', 0]], 'moveable': [['oven', 0], ['pizza', 2], ['pan', 2]], 'mixed': [['oven', 0], ['pizza', 0], ['pan', 0]], 'edible': [['oven', 0], ['pizza', 0], ['pan', 0]]}\n",
      "      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['knife', 2], ['slices', 2], ['hot pizza', 0]], 'exist': [['knife', 2], ['slices', 2], ['hot pizza', 2]], 'clean': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'power': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'functional': [['knife', 2], ['slices', 2], ['hot pizza', 2]], 'pieces': [['knife', 0], ['slices', 0], ['hot pizza', 4]], 'wet': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'open': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'temperature': [['knife', 0], ['slices', 0], ['hot pizza', 2]], 'solid': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'contain': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'running': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'moveable': [['knife', 2], ['slices', 2], ['hot pizza', 2]], 'mixed': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'edible': [['knife', 0], ['slices', 0], ['hot pizza', 0]]}\n",
      "    ],\n",
      "}\n",
      "\n",
      "\n",
      "{\n",
      "  story_id: \n",
      "    13,\n",
      "  worker_id: \n",
      "    A32W24TWSWXW,\n",
      "  type: \n",
      "    cloze,\n",
      "  idx: \n",
      "    0,\n",
      "  aug: \n",
      "    False,\n",
      "  actor: \n",
      "    John,\n",
      "  location: \n",
      "    kitchen,\n",
      "  objects: \n",
      "    cabinet, counter, knife, pan, potato, pizza,\n",
      "  sentences: \n",
      "    [\n",
      "      John was getting the snacks ready for the party.\n",
      "      John opened the cabinet, took out a pan and put it on the counter.\n",
      "      John opened the fridge and got out the pizza.\n",
      "      John put the pizza on the pan and put them into the oven.\n",
      "      John called the pizza joint to deliver a pizza.\n",
      "    ],\n",
      "  length: \n",
      "    5,\n",
      "  example_id: \n",
      "    13-C0,\n",
      "  plausible: \n",
      "    False,\n",
      "  breakpoint: \n",
      "    4,\n",
      "  confl_sents: \n",
      "    [\n",
      "      2\n",
      "      3\n",
      "    ],\n",
      "  confl_pairs: \n",
      "    [\n",
      "      [2, 4]\n",
      "      [3, 4]\n",
      "    ],\n",
      "  states: \n",
      "    [\n",
      "      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['snacks', 0], ['party', 0]], 'exist': [['snacks', 4], ['party', 2]], 'clean': [['snacks', 0], ['party', 0]], 'power': [['snacks', 0], ['party', 0]], 'functional': [['snacks', 2], ['party', 2]], 'pieces': [['snacks', 0], ['party', 0]], 'wet': [['snacks', 0], ['party', 0]], 'open': [['snacks', 0], ['party', 0]], 'temperature': [['snacks', 0], ['party', 0]], 'solid': [['snacks', 0], ['party', 0]], 'contain': [['snacks', 0], ['party', 0]], 'running': [['snacks', 0], ['party', 0]], 'moveable': [['snacks', 2], ['party', 2]], 'mixed': [['snacks', 0], ['party', 0]], 'edible': [['snacks', 0], ['party', 0]]}\n",
      "      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['counter', 0], ['pan', 7], ['cabinet', 0]], 'exist': [['counter', 2], ['pan', 2], ['cabinet', 2]], 'clean': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'power': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'functional': [['counter', 2], ['pan', 2], ['cabinet', 2]], 'pieces': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'wet': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'open': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'temperature': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'solid': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'contain': [['counter', 6], ['pan', 0], ['cabinet', 8]], 'running': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'moveable': [['counter', 0], ['pan', 2], ['cabinet', 0]], 'mixed': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'edible': [['counter', 0], ['pan', 0], ['cabinet', 0]]}\n",
      "      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['fridge', 0], ['pizza', 7]], 'exist': [['fridge', 2], ['pizza', 2]], 'clean': [['fridge', 0], ['pizza', 0]], 'power': [['fridge', 2], ['pizza', 0]], 'functional': [['fridge', 2], ['pizza', 2]], 'pieces': [['fridge', 0], ['pizza', 0]], 'wet': [['fridge', 0], ['pizza', 0]], 'open': [['fridge', 4], ['pizza', 0]], 'temperature': [['fridge', 0], ['pizza', 1]], 'solid': [['fridge', 0], ['pizza', 0]], 'contain': [['fridge', 8], ['pizza', 0]], 'running': [['fridge', 2], ['pizza', 0]], 'moveable': [['fridge', 2], ['pizza', 2]], 'mixed': [['fridge', 0], ['pizza', 0]], 'edible': [['fridge', 0], ['pizza', 0]]}\n",
      "      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['oven', 0], ['pizza', 3], ['pan', 6]], 'exist': [['oven', 2], ['pizza', 2], ['pan', 2]], 'clean': [['oven', 0], ['pizza', 0], ['pan', 0]], 'power': [['oven', 2], ['pizza', 0], ['pan', 0]], 'functional': [['oven', 2], ['pizza', 2], ['pan', 2]], 'pieces': [['oven', 0], ['pizza', 0], ['pan', 0]], 'wet': [['oven', 0], ['pizza', 0], ['pan', 0]], 'open': [['oven', 8], ['pizza', 0], ['pan', 0]], 'temperature': [['oven', 2], ['pizza', 0], ['pan', 0]], 'solid': [['oven', 0], ['pizza', 0], ['pan', 0]], 'contain': [['oven', 6], ['pizza', 0], ['pan', 4]], 'running': [['oven', 0], ['pizza', 0], ['pan', 0]], 'moveable': [['oven', 0], ['pizza', 2], ['pan', 2]], 'mixed': [['oven', 0], ['pizza', 0], ['pan', 0]], 'edible': [['oven', 0], ['pizza', 0], ['pan', 0]]}\n",
      "      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['pizza', 0]], 'exist': [['pizza', 4]], 'clean': [['pizza', 0]], 'power': [['pizza', 0]], 'functional': [['pizza', 2]], 'pieces': [['pizza', 0]], 'wet': [['pizza', 0]], 'open': [['pizza', 0]], 'temperature': [['pizza', 0]], 'solid': [['pizza', 0]], 'contain': [['pizza', 0]], 'running': [['pizza', 0]], 'moveable': [['pizza', 2]], 'mixed': [['pizza', 0]], 'edible': [['pizza', 0]]}\n",
      "    ],\n",
      "}\n",
      "\n",
      "\n",
      "{\n",
      "  story_id: \n",
      "    13,\n",
      "  worker_id: \n",
      "    A32W24TWSWXW,\n",
      "  type: \n",
      "    order,\n",
      "  idx: \n",
      "    2,\n",
      "  aug: \n",
      "    False,\n",
      "  actor: \n",
      "    John,\n",
      "  location: \n",
      "    kitchen,\n",
      "  objects: \n",
      "    cabinet, counter, knife, pan, potato, pizza,\n",
      "  sentences: \n",
      "    [\n",
      "      John was getting the snacks ready for the party.\n",
      "      John opened the cabinet, took out a pan and put it on the counter.\n",
      "      John put the pizza on the pan and put them into the oven.\n",
      "      John opened the fridge and got out the pizza.\n",
      "      John took a knife and cut the hot pizza in eight slices.\n",
      "    ],\n",
      "  length: \n",
      "    5,\n",
      "  example_id: \n",
      "    13-O2,\n",
      "  plausible: \n",
      "    False,\n",
      "  breakpoint: \n",
      "    3,\n",
      "  confl_sents: \n",
      "    [\n",
      "      2\n",
      "    ],\n",
      "  confl_pairs: \n",
      "    [],\n",
      "  states: \n",
      "    [\n",
      "      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['snacks', 0], ['party', 0]], 'exist': [['snacks', 4], ['party', 2]], 'clean': [['snacks', 0], ['party', 0]], 'power': [['snacks', 0], ['party', 0]], 'functional': [['snacks', 2], ['party', 2]], 'pieces': [['snacks', 0], ['party', 0]], 'wet': [['snacks', 0], ['party', 0]], 'open': [['snacks', 0], ['party', 0]], 'temperature': [['snacks', 0], ['party', 0]], 'solid': [['snacks', 0], ['party', 0]], 'contain': [['snacks', 0], ['party', 0]], 'running': [['snacks', 0], ['party', 0]], 'moveable': [['snacks', 2], ['party', 2]], 'mixed': [['snacks', 0], ['party', 0]], 'edible': [['snacks', 0], ['party', 0]]}\n",
      "      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['counter', 0], ['pan', 7], ['cabinet', 0]], 'exist': [['counter', 2], ['pan', 2], ['cabinet', 2]], 'clean': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'power': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'functional': [['counter', 2], ['pan', 2], ['cabinet', 2]], 'pieces': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'wet': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'open': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'temperature': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'solid': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'contain': [['counter', 6], ['pan', 0], ['cabinet', 8]], 'running': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'moveable': [['counter', 0], ['pan', 2], ['cabinet', 0]], 'mixed': [['counter', 0], ['pan', 0], ['cabinet', 0]], 'edible': [['counter', 0], ['pan', 0], ['cabinet', 0]]}\n",
      "      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['oven', 0], ['pizza', 3], ['pan', 6]], 'exist': [['oven', 2], ['pizza', 2], ['pan', 2]], 'clean': [['oven', 0], ['pizza', 0], ['pan', 0]], 'power': [['oven', 2], ['pizza', 0], ['pan', 0]], 'functional': [['oven', 2], ['pizza', 2], ['pan', 2]], 'pieces': [['oven', 0], ['pizza', 0], ['pan', 0]], 'wet': [['oven', 0], ['pizza', 0], ['pan', 0]], 'open': [['oven', 8], ['pizza', 0], ['pan', 0]], 'temperature': [['oven', 2], ['pizza', 0], ['pan', 0]], 'solid': [['oven', 0], ['pizza', 0], ['pan', 0]], 'contain': [['oven', 6], ['pizza', 0], ['pan', 4]], 'running': [['oven', 0], ['pizza', 0], ['pan', 0]], 'moveable': [['oven', 0], ['pizza', 2], ['pan', 2]], 'mixed': [['oven', 0], ['pizza', 0], ['pan', 0]], 'edible': [['oven', 0], ['pizza', 0], ['pan', 0]]}\n",
      "      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['fridge', 0], ['pizza', 7]], 'exist': [['fridge', 2], ['pizza', 2]], 'clean': [['fridge', 0], ['pizza', 0]], 'power': [['fridge', 2], ['pizza', 0]], 'functional': [['fridge', 2], ['pizza', 2]], 'pieces': [['fridge', 0], ['pizza', 0]], 'wet': [['fridge', 0], ['pizza', 0]], 'open': [['fridge', 4], ['pizza', 0]], 'temperature': [['fridge', 0], ['pizza', 1]], 'solid': [['fridge', 0], ['pizza', 0]], 'contain': [['fridge', 8], ['pizza', 0]], 'running': [['fridge', 2], ['pizza', 0]], 'moveable': [['fridge', 2], ['pizza', 2]], 'mixed': [['fridge', 0], ['pizza', 0]], 'edible': [['fridge', 0], ['pizza', 0]]}\n",
      "      {'h_location': [['John', 0]], 'conscious': [['John', 2]], 'wearing': [['John', 0]], 'h_wet': [['John', 0]], 'hygiene': [['John', 0]], 'location': [['knife', 2], ['slices', 2], ['hot pizza', 0]], 'exist': [['knife', 2], ['slices', 2], ['hot pizza', 2]], 'clean': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'power': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'functional': [['knife', 2], ['slices', 2], ['hot pizza', 2]], 'pieces': [['knife', 0], ['slices', 0], ['hot pizza', 4]], 'wet': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'open': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'temperature': [['knife', 0], ['slices', 0], ['hot pizza', 2]], 'solid': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'contain': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'running': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'moveable': [['knife', 2], ['slices', 2], ['hot pizza', 2]], 'mixed': [['knife', 0], ['slices', 0], ['hot pizza', 0]], 'edible': [['knife', 0], ['slices', 0], ['hot pizza', 0]]}\n",
      "    ],\n",
      "}\n",
      "\n",
      "\n",
      "{\n",
      "  story_id: \n",
      "    33,\n",
      "  worker_id: \n",
      "    A1F01FVEPYCPHO,\n",
      "  type: \n",
      "    None,\n",
      "  idx: \n",
      "    None,\n",
      "  aug: \n",
      "    False,\n",
      "  actor: \n",
      "    Mary,\n",
      "  location: \n",
      "    bathroom,\n",
      "  objects: \n",
      "    washing machine, cabinet, toothpaste, bleach, socks, mirror,\n",
      "  sentences: \n",
      "    [\n",
      "      Mary took off her socks.\n",
      "      Mary put the socks in the washing machine.\n",
      "      Mary opened the cabinet.\n",
      "      Mary took out the toothbrush and toothpaste.\n",
      "      Mary brushed her teeth while looking in the mirror.\n",
      "    ],\n",
      "  length: \n",
      "    5,\n",
      "  example_id: \n",
      "    33,\n",
      "  plausible: \n",
      "    True,\n",
      "  breakpoint: \n",
      "    -1,\n",
      "  confl_sents: \n",
      "    [],\n",
      "  confl_pairs: \n",
      "    [],\n",
      "  states: \n",
      "    [\n",
      "      {'h_location': [['Mary', 0]], 'conscious': [['Mary', 2]], 'wearing': [['Mary', 8]], 'h_wet': [['Mary', 0]], 'hygiene': [['Mary', 0]], 'location': [['socks', 5]], 'exist': [['socks', 2]], 'clean': [['socks', 0]], 'power': [['socks', 0]], 'functional': [['socks', 2]], 'pieces': [['socks', 0]], 'wet': [['socks', 0]], 'open': [['socks', 0]], 'temperature': [['socks', 0]], 'solid': [['socks', 0]], 'contain': [['socks', 0]], 'running': [['socks', 0]], 'moveable': [['socks', 2]], 'mixed': [['socks', 0]], 'edible': [['socks', 0]]}\n",
      "      {'h_location': [['Mary', 0]], 'conscious': [['Mary', 2]], 'wearing': [['Mary', 0]], 'h_wet': [['Mary', 0]], 'hygiene': [['Mary', 0]], 'location': [['socks', 6], ['washing machine', 0]], 'exist': [['socks', 2], ['washing machine', 2]], 'clean': [['socks', 0], ['washing machine', 0]], 'power': [['socks', 0], ['washing machine', 0]], 'functional': [['socks', 2], ['washing machine', 2]], 'pieces': [['socks', 0], ['washing machine', 0]], 'wet': [['socks', 0], ['washing machine', 0]], 'open': [['socks', 0], ['washing machine', 8]], 'temperature': [['socks', 0], ['washing machine', 0]], 'solid': [['socks', 0], ['washing machine', 0]], 'contain': [['socks', 0], ['washing machine', 6]], 'running': [['socks', 0], ['washing machine', 0]], 'moveable': [['socks', 2], ['washing machine', 2]], 'mixed': [['socks', 0], ['washing machine', 0]], 'edible': [['socks', 0], ['washing machine', 0]]}\n",
      "      {'h_location': [['Mary', 0]], 'conscious': [['Mary', 2]], 'wearing': [['Mary', 0]], 'h_wet': [['Mary', 0]], 'hygiene': [['Mary', 0]], 'location': [['cabinet', 0]], 'exist': [['cabinet', 2]], 'clean': [['cabinet', 0]], 'power': [['cabinet', 0]], 'functional': [['cabinet', 2]], 'pieces': [['cabinet', 0]], 'wet': [['cabinet', 0]], 'open': [['cabinet', 4]], 'temperature': [['cabinet', 0]], 'solid': [['cabinet', 0]], 'contain': [['cabinet', 0]], 'running': [['cabinet', 0]], 'moveable': [['cabinet', 2]], 'mixed': [['cabinet', 0]], 'edible': [['cabinet', 0]]}\n",
      "      {'h_location': [['Mary', 0]], 'conscious': [['Mary', 2]], 'wearing': [['Mary', 0]], 'h_wet': [['Mary', 0]], 'hygiene': [['Mary', 0]], 'location': [['toothbrush', 7], ['toothpaste', 7]], 'exist': [['toothbrush', 2], ['toothpaste', 2]], 'clean': [['toothbrush', 0], ['toothpaste', 0]], 'power': [['toothbrush', 0], ['toothpaste', 0]], 'functional': [['toothbrush', 2], ['toothpaste', 2]], 'pieces': [['toothbrush', 0], ['toothpaste', 0]], 'wet': [['toothbrush', 0], ['toothpaste', 0]], 'open': [['toothbrush', 0], ['toothpaste', 0]], 'temperature': [['toothbrush', 0], ['toothpaste', 0]], 'solid': [['toothbrush', 0], ['toothpaste', 0]], 'contain': [['toothbrush', 0], ['toothpaste', 0]], 'running': [['toothbrush', 0], ['toothpaste', 0]], 'moveable': [['toothbrush', 2], ['toothpaste', 2]], 'mixed': [['toothbrush', 0], ['toothpaste', 0]], 'edible': [['toothbrush', 0], ['toothpaste', 0]]}\n",
      "      {'h_location': [['Mary', 0]], 'conscious': [['Mary', 2]], 'wearing': [['Mary', 0]], 'h_wet': [['Mary', 0]], 'hygiene': [['Mary', 6]], 'location': [['mirror', 0], ['teeth', 0]], 'exist': [['mirror', 2], ['teeth', 2]], 'clean': [['mirror', 0], ['teeth', 6]], 'power': [['mirror', 0], ['teeth', 0]], 'functional': [['mirror', 2], ['teeth', 2]], 'pieces': [['mirror', 0], ['teeth', 0]], 'wet': [['mirror', 0], ['teeth', 0]], 'open': [['mirror', 0], ['teeth', 0]], 'temperature': [['mirror', 0], ['teeth', 0]], 'solid': [['mirror', 0], ['teeth', 0]], 'contain': [['mirror', 0], ['teeth', 0]], 'running': [['mirror', 0], ['teeth', 0]], 'moveable': [['mirror', 2], ['teeth', 0]], 'mixed': [['mirror', 0], ['teeth', 0]], 'edible': [['mirror', 0], ['teeth', 0]]}\n",
      "    ],\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from www.utils import print_dict\n",
    "\n",
    "partitions = ['train', 'dev', 'test']\n",
    "subtasks = ['cloze', 'order']\n",
    "\n",
    "# We can split the data into multiple json files later\n",
    "data_file = './all_data/www.json'\n",
    "with open(data_file, 'r') as f:\n",
    "  dataset = json.load(f)\n",
    "\n",
    "print('Preprocessed examples:')\n",
    "for ex_idx in [0,1,5,10]:\n",
    "  ex = dataset['dev'][list(dataset['dev'].keys())[ex_idx]]\n",
    "  print_dict(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfc94f27-532a-4cc4-b057-6c6c707c8673",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloze_dataset = {p: [] for p in dataset}\n",
    "order_dataset = {p: [] for p in dataset}\n",
    "\n",
    "for p in dataset:\n",
    "  for exid in dataset[p]:\n",
    "    ex = dataset[p][exid]\n",
    "\n",
    "    if ex['type'] == None:\n",
    "      continue\n",
    "    \n",
    "    ex_plaus = dataset[p][str(ex['story_id'])]\n",
    "\n",
    "    if ex['type'] == 'cloze':\n",
    "      cloze_dataset[p].append(ex)\n",
    "      cloze_dataset[p].append(ex_plaus) # For every implausible story, add a copy of its corresponding plausible story\n",
    "\n",
    "    # Exclude augmented ordering examples from dev and test, since the breakpoints aren't always accurate in those\n",
    "    elif ex['type'] == 'order' and not (p != 'train' and ex['aug']): \n",
    "      order_dataset[p].append(ex)\n",
    "      order_dataset[p].append(ex_plaus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ceadead0-29bd-4081-8738-9b42ac024e4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloze label distribution (train):\n",
      "[(1, 400), (0, 399)]\n",
      "Cloze label distribution (dev):\n",
      "[(0, 161), (1, 161)]\n",
      "Cloze label distribution (test):\n",
      "[(1, 176), (0, 175)]\n",
      "{\n",
      "  example_id: \n",
      "    0-C0,\n",
      "  stories: \n",
      "    [\n",
      "      {'story_id': 0, 'worker_id': 'A1F01FVEPYCPHO', 'type': 'cloze', 'idx': 0, 'aug': False, 'actor': 'Tom', 'location': 'kitchen', 'objects': 'dustbin, microwave, pan, plate, cereal, soup', 'sentences': ['Tom bought a new dustbin for the kitchen.', 'Tom threw a broken plate in the dustbin.', 'Tom got some soup from the fridge.', 'Tom put the soup in the microwave.', 'Tom ate the cold soup.'], 'length': 5, 'example_id': '0-C0', 'plausible': False, 'breakpoint': 4, 'confl_sents': [3], 'confl_pairs': [[3, 4]], 'states': [{'h_location': [['Tom', 0]], 'conscious': [['Tom', 2]], 'wearing': [['Tom', 0]], 'h_wet': [['Tom', 0]], 'hygiene': [['Tom', 0]], 'location': [['dustbin', 6]], 'exist': [['dustbin', 4]], 'clean': [['dustbin', 0]], 'power': [['dustbin', 0]], 'functional': [['dustbin', 2]], 'pieces': [['dustbin', 0]], 'wet': [['dustbin', 0]], 'open': [['dustbin', 0]], 'temperature': [['dustbin', 0]], 'solid': [['dustbin', 0]], 'contain': [['dustbin', 0]], 'running': [['dustbin', 0]], 'moveable': [['dustbin', 2]], 'mixed': [['dustbin', 0]], 'edible': [['dustbin', 0]]}, {'h_location': [['Tom', 0]], 'conscious': [['Tom', 2]], 'wearing': [['Tom', 0]], 'h_wet': [['Tom', 0]], 'hygiene': [['Tom', 0]], 'location': [['dustbin', 0], ['plate', 6]], 'exist': [['dustbin', 2], ['plate', 2]], 'clean': [['dustbin', 0], ['plate', 5]], 'power': [['dustbin', 0], ['plate', 0]], 'functional': [['dustbin', 2], ['plate', 1]], 'pieces': [['dustbin', 0], ['plate', 0]], 'wet': [['dustbin', 0], ['plate', 0]], 'open': [['dustbin', 0], ['plate', 0]], 'temperature': [['dustbin', 0], ['plate', 0]], 'solid': [['dustbin', 0], ['plate', 0]], 'contain': [['dustbin', 6], ['plate', 0]], 'running': [['dustbin', 0], ['plate', 0]], 'moveable': [['dustbin', 0], ['plate', 2]], 'mixed': [['dustbin', 0], ['plate', 0]], 'edible': [['dustbin', 0], ['plate', 0]]}, {'h_location': [['Tom', 0]], 'conscious': [['Tom', 2]], 'wearing': [['Tom', 0]], 'h_wet': [['Tom', 0]], 'hygiene': [['Tom', 0]], 'location': [['fridge', 0], ['soup', 2]], 'exist': [['fridge', 2], ['soup', 2]], 'clean': [['fridge', 0], ['soup', 0]], 'power': [['fridge', 0], ['soup', 0]], 'functional': [['fridge', 2], ['soup', 2]], 'pieces': [['fridge', 0], ['soup', 0]], 'wet': [['fridge', 0], ['soup', 0]], 'open': [['fridge', 8], ['soup', 0]], 'temperature': [['fridge', 0], ['soup', 1]], 'solid': [['fridge', 0], ['soup', 0]], 'contain': [['fridge', 8], ['soup', 0]], 'running': [['fridge', 0], ['soup', 0]], 'moveable': [['fridge', 2], ['soup', 2]], 'mixed': [['fridge', 0], ['soup', 0]], 'edible': [['fridge', 0], ['soup', 0]]}, {'h_location': [['Tom', 0]], 'conscious': [['Tom', 2]], 'wearing': [['Tom', 0]], 'h_wet': [['Tom', 0]], 'hygiene': [['Tom', 0]], 'location': [['microwave', 0], ['soup', 3]], 'exist': [['microwave', 2], ['soup', 2]], 'clean': [['microwave', 0], ['soup', 0]], 'power': [['microwave', 2], ['soup', 0]], 'functional': [['microwave', 2], ['soup', 2]], 'pieces': [['microwave', 0], ['soup', 0]], 'wet': [['microwave', 0], ['soup', 0]], 'open': [['microwave', 8], ['soup', 0]], 'temperature': [['microwave', 0], ['soup', 0]], 'solid': [['microwave', 0], ['soup', 0]], 'contain': [['microwave', 6], ['soup', 0]], 'running': [['microwave', 0], ['soup', 0]], 'moveable': [['microwave', 2], ['soup', 2]], 'mixed': [['microwave', 0], ['soup', 0]], 'edible': [['microwave', 0], ['soup', 0]]}, {'h_location': [['Tom', 0]], 'conscious': [['Tom', 2]], 'wearing': [['Tom', 0]], 'h_wet': [['Tom', 0]], 'hygiene': [['Tom', 0]], 'location': [['soup', 1]], 'exist': [['soup', 3]], 'clean': [['soup', 0]], 'power': [['soup', 0]], 'functional': [['soup', 2]], 'pieces': [['soup', 0]], 'wet': [['soup', 0]], 'open': [['soup', 0]], 'temperature': [['soup', 7]], 'solid': [['soup', 0]], 'contain': [['soup', 0]], 'running': [['soup', 0]], 'moveable': [['soup', 2]], 'mixed': [['soup', 0]], 'edible': [['soup', 0]]}]}\n",
      "      {'story_id': 0, 'worker_id': 'A1F01FVEPYCPHO', 'type': None, 'idx': None, 'aug': False, 'actor': 'Tom', 'location': 'kitchen', 'objects': 'dustbin, microwave, pan, plate, cereal, soup', 'sentences': ['Tom bought a new dustbin for the kitchen.', 'Tom threw a broken plate in the dustbin.', 'Tom got some soup from the fridge.', 'Tom put the soup in the microwave.', 'Tom turned on the microwave.'], 'length': 5, 'example_id': '0', 'plausible': True, 'breakpoint': -1, 'confl_sents': [], 'confl_pairs': [], 'states': [{'h_location': [['Tom', 0]], 'conscious': [['Tom', 2]], 'wearing': [['Tom', 0]], 'h_wet': [['Tom', 0]], 'hygiene': [['Tom', 0]], 'location': [['dustbin', 6]], 'exist': [['dustbin', 4]], 'clean': [['dustbin', 0]], 'power': [['dustbin', 0]], 'functional': [['dustbin', 2]], 'pieces': [['dustbin', 0]], 'wet': [['dustbin', 0]], 'open': [['dustbin', 0]], 'temperature': [['dustbin', 0]], 'solid': [['dustbin', 0]], 'contain': [['dustbin', 0]], 'running': [['dustbin', 0]], 'moveable': [['dustbin', 2]], 'mixed': [['dustbin', 0]], 'edible': [['dustbin', 0]]}, {'h_location': [['Tom', 0]], 'conscious': [['Tom', 2]], 'wearing': [['Tom', 0]], 'h_wet': [['Tom', 0]], 'hygiene': [['Tom', 0]], 'location': [['dustbin', 0], ['plate', 6]], 'exist': [['dustbin', 2], ['plate', 2]], 'clean': [['dustbin', 0], ['plate', 5]], 'power': [['dustbin', 0], ['plate', 0]], 'functional': [['dustbin', 2], ['plate', 1]], 'pieces': [['dustbin', 0], ['plate', 0]], 'wet': [['dustbin', 0], ['plate', 0]], 'open': [['dustbin', 0], ['plate', 0]], 'temperature': [['dustbin', 0], ['plate', 0]], 'solid': [['dustbin', 0], ['plate', 0]], 'contain': [['dustbin', 6], ['plate', 0]], 'running': [['dustbin', 0], ['plate', 0]], 'moveable': [['dustbin', 0], ['plate', 2]], 'mixed': [['dustbin', 0], ['plate', 0]], 'edible': [['dustbin', 0], ['plate', 0]]}, {'h_location': [['Tom', 0]], 'conscious': [['Tom', 2]], 'wearing': [['Tom', 0]], 'h_wet': [['Tom', 0]], 'hygiene': [['Tom', 0]], 'location': [['fridge', 0], ['soup', 2]], 'exist': [['fridge', 2], ['soup', 2]], 'clean': [['fridge', 0], ['soup', 0]], 'power': [['fridge', 0], ['soup', 0]], 'functional': [['fridge', 2], ['soup', 2]], 'pieces': [['fridge', 0], ['soup', 0]], 'wet': [['fridge', 0], ['soup', 0]], 'open': [['fridge', 8], ['soup', 0]], 'temperature': [['fridge', 0], ['soup', 1]], 'solid': [['fridge', 0], ['soup', 0]], 'contain': [['fridge', 8], ['soup', 0]], 'running': [['fridge', 0], ['soup', 0]], 'moveable': [['fridge', 2], ['soup', 2]], 'mixed': [['fridge', 0], ['soup', 0]], 'edible': [['fridge', 0], ['soup', 0]]}, {'h_location': [['Tom', 0]], 'conscious': [['Tom', 2]], 'wearing': [['Tom', 0]], 'h_wet': [['Tom', 0]], 'hygiene': [['Tom', 0]], 'location': [['microwave', 0], ['soup', 3]], 'exist': [['microwave', 2], ['soup', 2]], 'clean': [['microwave', 0], ['soup', 0]], 'power': [['microwave', 2], ['soup', 0]], 'functional': [['microwave', 2], ['soup', 2]], 'pieces': [['microwave', 0], ['soup', 0]], 'wet': [['microwave', 0], ['soup', 0]], 'open': [['microwave', 8], ['soup', 0]], 'temperature': [['microwave', 0], ['soup', 0]], 'solid': [['microwave', 0], ['soup', 0]], 'contain': [['microwave', 6], ['soup', 0]], 'running': [['microwave', 0], ['soup', 0]], 'moveable': [['microwave', 2], ['soup', 2]], 'mixed': [['microwave', 0], ['soup', 0]], 'edible': [['microwave', 0], ['soup', 0]]}, {'h_location': [['Tom', 0]], 'conscious': [['Tom', 2]], 'wearing': [['Tom', 0]], 'h_wet': [['Tom', 0]], 'hygiene': [['Tom', 0]], 'location': [['microwave', 0]], 'exist': [['microwave', 2]], 'clean': [['microwave', 0]], 'power': [['microwave', 2]], 'functional': [['microwave', 2]], 'pieces': [['microwave', 0]], 'wet': [['microwave', 0]], 'open': [['microwave', 1]], 'temperature': [['microwave', 0]], 'solid': [['microwave', 0]], 'contain': [['microwave', 2]], 'running': [['microwave', 4]], 'moveable': [['microwave', 2]], 'mixed': [['microwave', 0]], 'edible': [['microwave', 0]]}]}\n",
      "    ],\n",
      "  length: \n",
      "    5,\n",
      "  label: \n",
      "    1,\n",
      "  breakpoint: \n",
      "    4,\n",
      "  confl_sents: \n",
      "    [\n",
      "      3\n",
      "    ],\n",
      "  confl_pairs: \n",
      "    [\n",
      "      [3, 4]\n",
      "    ],\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from www.utils import print_dict\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "data_file = './all_data/www_2s_new.json'\n",
    "with open(data_file, 'r') as f:\n",
    "  cloze_dataset_2s, order_dataset_2s = json.load(f)  \n",
    "\n",
    "for p in cloze_dataset_2s:\n",
    "  label_dist = Counter([ex['label'] for ex in cloze_dataset_2s[p]])\n",
    "  print('Cloze label distribution (%s):' % p)\n",
    "  print(label_dist.most_common())\n",
    "print_dict(cloze_dataset_2s['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c1af4d-8ccd-46aa-87d0-a86bb73d9626",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from www.dataset.prepro import get_tiered_data, balance_labels\n",
    "from collections import Counter\n",
    "tiered_dataset = cloze_dataset_2s\n",
    "\n",
    "# Debug the code on a small amount of data\n",
    "if False:\n",
    "    for k in tiered_dataset:\n",
    "        tiered_dataset[k] = tiered_dataset[k][:5]\n",
    "\n",
    "maxStoryLength=168       \n",
    "tiered_dataset = get_tiered_data(tiered_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf0eb76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa0e4c6e",
   "metadata": {},
   "source": [
    "### Object Abstraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dffc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_file, 'r') as f:\n",
    "    new_cloze_dataset_2s, order_dataset_2s = json.load(f)  \n",
    "    \n",
    "abstract_dataset = new_cloze_dataset_2s\n",
    "maxStoryLength=168      \n",
    "\n",
    "if False:\n",
    "    for k in abstract_dataset:\n",
    "        abstract_dataset[k] = abstract_dataset[k][:5]\n",
    "\n",
    "abstract_dataset = get_tiered_data(abstract_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263b92b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./abstract_tree.pkl\", \"rb\") as tf:\n",
    "    abstract_dict = pickle.load(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e44ce0-757f-4438-9a03-6e591a5dfbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72ae9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in abstract_dataset.keys():\n",
    "    for data in tqdm(abstract_dataset[key]):\n",
    "        for sample in data['stories']:\n",
    "            original_sentences=sample['sentences']\n",
    "            entity_list = [entity['entity'] for entity in sample['entities']]\n",
    "            abstract_sentences=[]\n",
    "            for sent in original_sentences:\n",
    "                abstract_sent=sent\n",
    "                for entity in entity_list:\n",
    "                    if entity in abstract_dict.keys() and abstract_dict[entity]['parent']!='entity.n.01':\n",
    "                        parent_class = abstract_dict[entity]['parent'].split(\".\")[0]\n",
    "                        abstract_sent=abstract_sent.replace(entity,parent_class)\n",
    "                abstract_sentences.append(abstract_sent)\n",
    "            sample['sentences']=abstract_sentences\n",
    "            for ent in sample['entities']:\n",
    "                ent['sentences']=abstract_sentences\n",
    "                if ent['entity'] in abstract_dict.keys() and abstract_dict[ent['entity']]['parent']!='entity.n.01':\n",
    "                    ent['entity']= abstract_dict[ent['entity']]['parent'].split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69c7077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e40b7f2",
   "metadata": {},
   "source": [
    "### Synonym Replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d757b904",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_file, 'r') as f:\n",
    "    new_cloze_dataset_2s, order_dataset_2s = json.load(f)  \n",
    "    \n",
    "replace_dataset = new_cloze_dataset_2s\n",
    "maxStoryLength=168      \n",
    "\n",
    "if False:\n",
    "    for k in replace_dataset:\n",
    "        replace_dataset[k] = replace_dataset[k][:5]\n",
    "\n",
    "replace_dataset = get_tiered_data(replace_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646b0fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.int = np.int64\n",
    "np.float = np.float64\n",
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "import nlpaug.flow as nafc\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865794f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonym_replacement(sentence, entity_list, aug):\n",
    "    doc = nlp(sentence)\n",
    "    words = []\n",
    "    for token in doc:\n",
    "        if token.pos_ in ['VERB', 'ADJ', 'ADV'] and token.text not in entity_list:\n",
    "            if token.tag_ in ['VBD', 'VBN']:\n",
    "                lemma_word = token.lemma_\n",
    "                augmented_word = aug.augment(lemma_word)[0]\n",
    "            else:\n",
    "                augmented_word = aug.augment(token.text)[0]\n",
    "            words.append(augmented_word)\n",
    "        else:\n",
    "            words.append(token.text)\n",
    "    augmented_sentence = ' '.join(words)\n",
    "    \n",
    "    return augmented_sentence\n",
    "\n",
    "\n",
    "def augment_story_data(dataset, aug):\n",
    "    for story_pair in tqdm(dataset):\n",
    "        story_1 = story_pair['stories'][0]\n",
    "        story_2 = story_pair['stories'][1]\n",
    "\n",
    "        entity_list_1 = [_['entity'] for _ in story_1['entities']] + [story_1['actor']] + [story_1['location']] + [story_1['objects']]\n",
    "        entity_list_2 = [_['entity'] for _ in story_2['entities']] + [story_2['actor']] + [story_2['location']] + [story_2['objects']]\n",
    "\n",
    "        aug_sentence_list_1 = []\n",
    "        aug_sentence_list_2 = []\n",
    "        \n",
    "        for sentence1, sentence2 in zip(story_1['sentences'], story_2['sentences']):\n",
    "            aug_sentence_1 = synonym_replacement(sentence1, entity_list_1, aug)\n",
    "            aug_sentence_list_1.append(aug_sentence_1)\n",
    "            aug_sentence_2 = synonym_replacement(sentence2, entity_list_2, aug)\n",
    "            aug_sentence_list_2.append(aug_sentence_2)\n",
    "\n",
    "        story_1['sentences'] = aug_sentence_list_1\n",
    "        story_2['sentences'] = aug_sentence_list_2\n",
    "\n",
    "        for entity in story_1['entities']:\n",
    "            entity['sentences'] = aug_sentence_list_1\n",
    "        for entity in story_2['entities']:\n",
    "            entity['sentences'] = aug_sentence_list_2\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838869a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = naw.SynonymAug(p = 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b67934",
   "metadata": {},
   "outputs": [],
   "source": [
    "syno_dataset = {}\n",
    "syno_dataset['train'] = augment_story_data(replace_dataset['train'], aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e723d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f6fa9a2",
   "metadata": {},
   "source": [
    "### GITA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7a96cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./all_data/GITA_test.json', 'r') as f:\n",
    "    gita_dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4e6136",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "model_name = 'Helsinki-NLP/opus-mt-it-en'\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493505b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def translate_text(text, model, tokenizer):\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\", padding=True)\n",
    "    translated = model.generate(inputs, max_length=50)\n",
    "    return tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "\n",
    "def translate_sample(sample, model, tokenizer):\n",
    "    trans_sentences = [translate_text(sentence, model, tokenizer) for sentence in sample['sentences']]\n",
    "    location = sample['location']\n",
    "    objects = sample['objects'].split(', ')\n",
    "    trans_location = translate_text(location, model, tokenizer)\n",
    "    trans_objects = {obj.strip(): translate_text(obj.strip(), model, tokenizer) for obj in objects}\n",
    "    trans_sample = copy.deepcopy(sample)\n",
    "    trans_sample['sentences'] = trans_sentences\n",
    "    trans_sample['location'] = trans_location\n",
    "    trans_sample['objects'] = ', '.join([trans_objects[obj] for obj in objects])\n",
    "    for state in trans_sample['states']:\n",
    "        for key in state:\n",
    "            for i in range(len(state[key])):\n",
    "                obj, val = state[key][i]\n",
    "                obj = obj.strip()\n",
    "                if obj in trans_objects:\n",
    "                    trans_obj = trans_objects[obj]\n",
    "                    state[key][i][0] = trans_obj\n",
    "    return trans_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b30536",
   "metadata": {},
   "outputs": [],
   "source": [
    "samplekeys = list(gita_dataset['test'].keys())\n",
    "translated_gita_dataset = {}\n",
    "translated_gita_dataset['test'] = {}\n",
    "for key in samplekeys:\n",
    "    sample = gita_dataset['test'][key]\n",
    "    trans_sample = translate_sample(sample, model, tokenizer)\n",
    "    translated_gita_dataset['test'][key] = trans_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0738f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./all_data/translated_gita.json', 'w') as f:\n",
    "    json.dump(translated_gita_dataset, f, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39995c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gita_dataset_2s = {}\n",
    "gita_dataset_2s['test'] = []\n",
    "for exid in range(117):\n",
    "    ex_plaus = gita_dataset['test'][str(exid)]  \n",
    "    if f'{exid}-C0' in gita_dataset['test']:\n",
    "        ex_implaus = gita_dataset['test'][f'{exid}-C0']\n",
    "        ex = {}\n",
    "        ex['example_id'] = f'{exid}-C0'\n",
    "        ex['stories'] = [ex_plaus, ex_implaus]\n",
    "        ex['length'] = 5\n",
    "        ex['label'] = 0\n",
    "        ex['breakpoint'] = ex_implaus['breakpoint']\n",
    "        if type(ex_implaus['confl_sents'][0]) == list:\n",
    "            ex['confl_sents'] = ex_implaus['confl_sents'][0]\n",
    "        else:\n",
    "            ex['confl_sents'] = ex_implaus['confl_sents']\n",
    "        if len(ex_implaus['confl_pairs']) > 1:\n",
    "            ex['confl_pairs'] = [[ex_implaus['confl_pairs'][0][0], ex_implaus['confl_pairs'][1][0]]]\n",
    "        else:\n",
    "            ex['confl_pairs'] = ex_implaus['confl_pairs']\n",
    "        gita_dataset_2s['test'].append(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebcb764",
   "metadata": {},
   "outputs": [],
   "source": [
    "gita_dataset_2s['test'][2]['confl_pairs'] = [[3,4]]\n",
    "gita_dataset_2s['test'][10]['confl_pairs'] = [[1,2]]\n",
    "gita_dataset_2s['test'].pop(74)\n",
    "gita_dataset_2s['test'].pop(69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3947ba99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a67bb61-4b14-417f-a90d-4f49d82ffc3d",
   "metadata": {},
   "source": [
    "### Knowledge Graph Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb9c2430-2291-4f52-9ffc-617af659b956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import conceptnet5\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7b95339-d5b1-4758-af67-efb476b06d01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "comet_model = AutoModelForSeq2SeqLM.from_pretrained(\"mismayil/comet-bart-ai2\")\n",
    "comet_tokenizer = AutoTokenizer.from_pretrained(\"mismayil/comet-bart-ai2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd6f96e0-6ebe-4db0-abde-e09f75d2b976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [ent.text for ent in doc.ents if ent.label_ in [\"PERSON\", \"GPE\", \"ORG\", \"PRODUCT\", \"FAC\", \"MISC\"]]\n",
    "    for chunk in doc.noun_chunks:\n",
    "        head = chunk.root\n",
    "        if head.pos_ == \"NOUN\":\n",
    "            entities.append(head.text)\n",
    "    return list(set(entities))\n",
    "\n",
    "def get_conceptnet_relations(entity):\n",
    "    base_url = \"http://api.conceptnet.io/c/en/\"\n",
    "    url = f\"{base_url}{entity}\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error querying ConceptNet API: {e}\")\n",
    "        return []\n",
    "    \n",
    "    relations = []\n",
    "    for edge in data.get(\"edges\", []):\n",
    "        weight = edge.get(\"weight\", 0)\n",
    "        if weight > 1:\n",
    "            start = edge[\"start\"][\"@id\"]\n",
    "            end = edge[\"end\"][\"@id\"]\n",
    "            relation = edge[\"rel\"][\"label\"]\n",
    "\n",
    "            if start.startswith(\"/c/en/\") and end.startswith(\"/c/en/\"):\n",
    "                start_label = edge[\"start\"][\"label\"]\n",
    "                end_label = edge[\"end\"][\"label\"]\n",
    "                relations.append((start_label, relation, end_label))\n",
    "    \n",
    "    return relations\n",
    "\n",
    "def generate_comet_knowledge(input_text, category=\"xAttr\", max_length=50):\n",
    "    prompt = f\"{input_text} {category}\"\n",
    "    inputs = comet_tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "    outputs = comet_model.generate(inputs[\"input_ids\"], max_length=max_length, num_beams=5)\n",
    "    generated_text = comet_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "def augment_sentence_with_knowledge(sentence, use_comet=False):\n",
    "    entities = extract_entities(sentence)\n",
    "\n",
    "    conceptnet_info = []\n",
    "    comet_info = []\n",
    "    \n",
    "    for entity in entities:\n",
    "        conceptnet_relations = get_conceptnet_relations(entity)\n",
    "        for rel in conceptnet_relations:\n",
    "            conceptnet_info.append(f\"{entity} is related to {rel[2]}.\")\n",
    "        \n",
    "        if use_comet:\n",
    "            xAttr_knowledge = generate_comet_knowledge(entity, category=\"xAttr\")\n",
    "            xEffect_knowledge = generate_comet_knowledge(entity, category = 'xEffect')\n",
    "            oEffect_knowledge = generate_comet_knowledge(entity, category=\"oEffect\")\n",
    "            comet_info.append(f\"For {entity}: xAttr = {xAttr_knowledge}, xEffect = {xEffect_knowledge}, oEffect = {oEffect_knowledge}.\")\n",
    "    \n",
    "    all_info = conceptnet_info + comet_info\n",
    "    unique_info = list(set(all_info))\n",
    "    descriptions = \" \".join(unique_info)\n",
    "    augmented_sentence = sentence + \" (\" + descriptions + \")\"\n",
    "    \n",
    "    return augmented_sentence\n",
    "\n",
    "def augment_sentences_with_knowledge(sentences, use_comet=False):\n",
    "    augmented_sentences = []\n",
    "    for sentence in sentences:\n",
    "        augmented_sentence = augment_sentence_with_knowledge(sentence, use_comet)\n",
    "        augmented_sentences.append(augmented_sentence)\n",
    "    \n",
    "    return augmented_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da6a20b8-2369-4bdb-8b7c-356bc1e1c9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_description(dataset):\n",
    "    newdataset = dataset.copy()\n",
    "    for i in range(len(newdataset['train'])):\n",
    "        stories = newdataset['train'][i]['stories']\n",
    "        for j in range(len(stories)):\n",
    "            sentences = stories[j]['sentences']\n",
    "            augmented_sentences = augment_sentences_with_knowledge(sentences, use_comet = True)\n",
    "            #print(augmented_sentences)\n",
    "            newdataset['train'][i]['stories'][j]['sentences'] = augmented_sentences\n",
    "    return newdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09495d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_dataset = add_description(kg_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98517037-fce0-4e54-ba59-b16766c5c4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"./all_data/kg_aug_data.json\", \"w\") as json_file:\n",
    "    json.dump(kg_dataset, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a30288-721f-4dd3-8cf3-64e0a1e096cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2630d27c",
   "metadata": {},
   "source": [
    "### Process data for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6357477a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from www.dataset.prepro_for_aug import get_tiered_data, balance_labels\n",
    "from www.dataset.featurize_for_aug import add_bert_features_tiered, get_tensor_dataset_tiered\n",
    "from collections import Counter\n",
    "\n",
    "gita_tiered_dataset = gita_dataset_2s\n",
    "gita_tiered_dataset = get_tiered_data(gita_tiered_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1b6743",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_tiered_dataset = {}\n",
    "aug_tiered_dataset['train'] = tiered_dataset['train'] + gita_tiered_dataset['test']\n",
    "aug_tiered_dataset['dev'] = tiered_dataset['dev']\n",
    "aug_tiered_dataset['test'] = tiered_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1e7acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spans = False\n",
    "seq_length = 16 # Max sequence length to pad to\n",
    "\n",
    "aug_tiered_dataset = add_bert_features_tiered(aug_tiered_dataset, tokenizer, seq_length, add_segment_ids=True)\n",
    "\n",
    "aug_tiered_tensor_dataset = {}\n",
    "max_story_length = 7\n",
    "for p in aug_tiered_dataset:\n",
    "    aug_tiered_tensor_dataset[p] = get_tensor_dataset_tiered(aug_tiered_dataset[p], max_story_length, add_segment_ids = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
